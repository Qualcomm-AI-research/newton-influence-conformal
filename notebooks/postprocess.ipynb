{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd51f606-d808-485a-8704-f7ea646f64ca",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Qualcomm Technologies, Inc.\n",
    "All Rights Reserved.\n",
    "\n",
    "# Notebook for collecting and analyzing results\n",
    "\n",
    "The scripts `run_uci.py` and `run_pets.py` store results in a `.npz` file format. This notebook demonstrates how to parse the results stored in those files and analyze if the observed coverage is statistically valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcbaf66-c774-4b56-8a4d-95353831b454",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4f392-0fde-42db-9a99-c3658f5b3177",
   "metadata": {},
   "source": [
    "## Functions used to collect saved results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571a0637-6367-4dbc-9709-a05b215fe2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_n_seeds(name: str) -> int:\n",
    "    \"\"\"Return the number random seeds used for each dataset.\n",
    "    We use 10 random seeds for the smallest datasets in the list\n",
    "    below and 20 for the remaining ones.\n",
    "    \"\"\"\n",
    "    return 10 if name in [\"boston\", \"concrete\", \"energy\", \"wine\", \"yacht\"] else 20\n",
    "\n",
    "\n",
    "def get_ds_hyperparam_type(name: str) -> str:\n",
    "    \"\"\"Return the type of hyperparameters optimization used for each dataset.\"\"\"\n",
    "    return (\n",
    "        \"marglik\"\n",
    "        if name in [\"boston\", \"concrete\", \"energy\", \"kin8nm\", \"power\", \"wine\", \"yacht\"]\n",
    "        else \"cv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_results_helper(\n",
    "    dataset_name: str,\n",
    "    results_path: str,\n",
    "    split: bool = False,\n",
    "    split_calib_size: Optional[float] = None,\n",
    "    hyperparam_type: Optional[str] = None,\n",
    "    seeds: Optional[List[int]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Load the corresponding file with the results of a UCI experiment and\n",
    "    parse it to a python dictionary.\"\"\"\n",
    "    # Get file extension according to hyperparameter optimization strategy.\n",
    "    if hyperparam_type is None:\n",
    "        ext = f\"_{get_ds_hyperparam_type(dataset_name)}\"\n",
    "    else:\n",
    "        ext = hyperparam_type\n",
    "    # If seeds not provided, define them as in the paper.\n",
    "    if seeds is None:\n",
    "        n_seeds = get_ds_n_seeds(dataset_name)\n",
    "        seeds = np.arange(n_seeds) + 1\n",
    "    # Collect results.\n",
    "    res = []\n",
    "    for seed in seeds:\n",
    "        if split_calib_size is None and split is False:\n",
    "            fn = os.path.join(results_path, dataset_name, f\"{dataset_name}{ext}_{seed}.npz\")\n",
    "        elif split is False:\n",
    "            fn = os.path.join(\n",
    "                results_path,\n",
    "                dataset_name,\n",
    "                f\"{dataset_name}{ext}_full{int(split_calib_size*100)}_{seed}.npz\",\n",
    "            )\n",
    "        elif split is True:\n",
    "            fn = os.path.join(\n",
    "                results_path,\n",
    "                dataset_name,\n",
    "                f\"{dataset_name}{ext}_split{int(split_calib_size*100)}_{seed}.npz\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unrecognised args\")\n",
    "        try:\n",
    "            res_per_rep = np.load(fn)\n",
    "        except:\n",
    "            raise FileNotFoundError(f\"Missing file {fn}\")\n",
    "        # Add results for that seed to the list\n",
    "        res.append(\n",
    "            {\n",
    "                \"cp_intervals\": res_per_rep[\"cp_intervals\"],\n",
    "                \"interval_widths_avg\": res_per_rep[\"interval_widths_avg\"].mean(axis=-1),\n",
    "                \"coverage\": res_per_rep[\"coverage\"].mean(axis=-1),\n",
    "                \"test_mse\": res_per_rep[\"test_mse\"].mean(),\n",
    "                \"test_loglik\": res_per_rep[\"test_loglik\"].mean(),\n",
    "                \"test_loglik_bayes\": res_per_rep[\"test_loglik_bayes\"].mean(),\n",
    "            }\n",
    "        )\n",
    "    # Create dictionary from list of results\n",
    "    res = {k: np.array([dd[k] for dd in res]) for k in res[0]}\n",
    "    res[\"cp_intervals\"] = np.moveaxis(\n",
    "        res[\"cp_intervals\"], 0, -1\n",
    "    )  # NOTE: cannot take mean here as different individual test points\n",
    "    res[\"interval_widths_avg\"] = np.moveaxis(res[\"interval_widths_avg\"], 0, -1)\n",
    "    res[\"coverage\"] = np.moveaxis(res[\"coverage\"], 0, -1)\n",
    "    if split:\n",
    "        res[\"methods\"] = [\n",
    "            method + f\"_{int(split_calib_size*100)}\" for method in res_per_rep[\"methods\"]\n",
    "        ]\n",
    "    else:\n",
    "        res[\"methods\"] = res_per_rep[\"methods\"].tolist()\n",
    "    res[\"sig_lvls\"] = res_per_rep[\"sig_lvls\"]\n",
    "    res[\"dataset\"] = dataset_name\n",
    "    res[\"split_calib_size\"] = split_calib_size\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_results_helper_pets(\n",
    "    results_path: str,\n",
    "    cp_type: str,\n",
    "    split_calib_size: float = 0.25,\n",
    "    seeds: Optional[List[int]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Load the corresponding file with the results of an Oxford Pets experiment\n",
    "    and parse it to a python dictionary.\"\"\"\n",
    "    if seeds is None:\n",
    "        seeds = np.arange(20) + 1\n",
    "    res = []\n",
    "    for seed in seeds:\n",
    "        if cp_type == \"full\":\n",
    "            fn = os.path.join(results_path, f\"pet_{seed}.npz\")\n",
    "        elif cp_type == \"full_refine\":\n",
    "            fn = os.path.join(results_path, f\"pet_full{int(split_calib_size*100)}_{seed}.npz\")\n",
    "        elif cp_type == \"split\":\n",
    "            fn = os.path.join(results_path, f\"pet_split{int(split_calib_size*100)}_{seed}.npz\")\n",
    "        else:\n",
    "            raise ValueError(\"unrecognised args\")\n",
    "\n",
    "        try:\n",
    "            res_per_rep = np.load(fn)\n",
    "        except FileNotFoundError:\n",
    "            print(fn)\n",
    "        else:\n",
    "            res.append(\n",
    "                {\n",
    "                    \"volumes\": res_per_rep[\"volumes\"],\n",
    "                    \"coverage\": res_per_rep[\"coverage\"],\n",
    "                    \"test_loc_error\": res_per_rep[\"test_loc_error\"].item(),\n",
    "                    \"test_acc\": res_per_rep[\"test_acc\"].item(),\n",
    "                }\n",
    "            )\n",
    "    res = {k: np.array([dd[k] for dd in res]) for k in res[0]}\n",
    "    res[\"volumes\"] = np.moveaxis(res[\"volumes\"], 0, -1)\n",
    "    res[\"interval_widths_avg\"] = res[\"volumes\"]\n",
    "    res[\"coverage\"] = np.moveaxis(res[\"coverage\"], 0, -1)\n",
    "\n",
    "    res[\"sig_lvls\"] = res_per_rep[\"sig_lvls\"]\n",
    "\n",
    "    if cp_type in [\"split\", \"full_refine\"]:\n",
    "        res[\"methods\"] = [\n",
    "            method + f\"_{int(split_calib_size*100)}\" for method in res_per_rep[\"methods\"]\n",
    "        ]\n",
    "    else:\n",
    "        res[\"methods\"] = res_per_rep[\"methods\"].tolist()\n",
    "    res[\"dataset\"] = \"pets\"\n",
    "    res[\"split_calib_size\"] = split_calib_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c85c1-9ab6-43e9-bf02-d684f5d3bec6",
   "metadata": {},
   "source": [
    "## Function used to evaluate valid coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804568dd-1800-42e7-8b2c-007d28a2afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set sizes (in full methods) used to evaluate exact marginal coverage distribution\n",
    "get_ds_full_n = {\n",
    "    \"yacht\": 277,\n",
    "    \"boston\": 455,\n",
    "    \"energy\": 691,\n",
    "    \"bike\": 9797,\n",
    "    \"protein\": 41157,\n",
    "    \"facebook_2\": 73179,\n",
    "    \"concrete\": 927,\n",
    "    \"wine\": 1439,\n",
    "    \"kin8nm\": 7372,\n",
    "    \"power\": 8611,\n",
    "    \"community\": 1794,\n",
    "    \"facebook_1\": 36853,\n",
    "    \"pets\": 7349,\n",
    "}\n",
    "\n",
    "\n",
    "def cov_check(\n",
    "    dataset_name, sig_lvl: float, method: str, emp_cov: float, split_calib_size: float\n",
    ") -> bool:\n",
    "    \"\"\"Check if the observed coverage is statistically valid given the dataset size.\"\"\"\n",
    "    if method in [\"bayes\", \"crr_studentized\", \"crr_standard\", \"crr_deleted\"]:\n",
    "        n_calib = get_ds_full_n[dataset_name]\n",
    "    else:\n",
    "        n_calib = int(1 + get_ds_full_n[dataset_name] * split_calib_size)\n",
    "    l = np.floor((n_calib + 1) * sig_lvl).astype(int)\n",
    "    beta_a = n_calib + 1 - l\n",
    "    beta_b = l\n",
    "    cov_lower = sp.stats.beta.ppf(0.01, a=beta_a, b=beta_b)\n",
    "    cov_upper = sp.stats.beta.ppf(0.99, a=beta_a, b=beta_b)\n",
    "    if (emp_cov >= cov_lower) and (emp_cov <= cov_upper):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854954b-3973-42c4-aa41-89801aea1cc9",
   "metadata": {},
   "source": [
    "## Print function that computes mean and standard deviation and checks for valid coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663501a4-21fe-4ddb-b5ad-93c543c3b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(res: dict):\n",
    "    \"\"\"Print the results from a dictionary created by a get_results_helper function.\"\"\"\n",
    "    # Coverage and width results have shape (num_methods, num_sig_lvls, num_seeds)\n",
    "    for sig_lvl_idx, sig_lvl in enumerate(res[\"sig_lvls\"]):\n",
    "        print(f\"Confidence level of {1-sig_lvl} \\tAvg. Coverage \\t\\tAvg. Size\")\n",
    "        for method_idx, method in enumerate(res[\"methods\"]):\n",
    "            mu_cov = res[\"coverage\"][method_idx, sig_lvl_idx, :].mean()\n",
    "            sigma_cov = res[\"coverage\"][method_idx, sig_lvl_idx, :].std()\n",
    "            mu_size = np.mean(res[\"interval_widths_avg\"][method_idx, sig_lvl_idx, :])\n",
    "            sigma_size = np.ma.std(res[\"interval_widths_avg\"][method_idx, sig_lvl_idx, :])\n",
    "            if isinstance(sigma_size, np.ma.core.MaskedConstant):\n",
    "                sigma_size = 0.0\n",
    "            # Check if we get valid coverage\n",
    "            valid_cov = cov_check(res[\"dataset\"], sig_lvl, method, mu_cov, res[\"split_calib_size\"])\n",
    "            checkmark = \"\\u2713\" if valid_cov else \"\\u2717\"\n",
    "            print(\n",
    "                f\"\\t{method:<20} \\t{mu_cov:>.4f} +/- {sigma_cov:>.4f} {checkmark} \\t{mu_size:>.4f} +/- {sigma_size:>.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecc670-f716-48f9-8701-8040872ad4b2",
   "metadata": {},
   "source": [
    "## Example UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514d43bd-a8aa-4045-a7c2-7737030d0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SPLIT_CALIB_SIZE = 0.5\n",
    "RES_PATH = \"../results/uci/\"  # Change the path as needed\n",
    "DS_NAME = \"boston\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b60fd99-19da-4867-8003-b15645d949e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.8956 +/- 0.0134 ✓ \t10.6348 +/- 0.3888\n",
      "\tscp_norm_var_50      \t0.8974 +/- 0.0136 ✓ \t12.6299 +/- 1.1968\n",
      "\tscp_norm_std_50      \t0.8952 +/- 0.0157 ✓ \t10.3008 +/- 0.2828\n",
      "\tscp_cqr_50           \t0.9010 +/- 0.0106 ✓ \t11.6923 +/- 0.4083\n",
      "\tscp_crf_50           \t0.8992 +/- 0.0098 ✓ \t43.4664 +/- 94.6179\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.9464 +/- 0.0103 ✓ \t14.5094 +/- 0.5408\n",
      "\tscp_norm_var_50      \t0.9442 +/- 0.0117 ✓ \t16.2063 +/- 1.5696\n",
      "\tscp_norm_std_50      \t0.9482 +/- 0.0102 ✓ \t13.4183 +/- 0.4780\n",
      "\tscp_cqr_50           \t0.9512 +/- 0.0077 ✓ \t15.1151 +/- 0.6727\n",
      "\tscp_crf_50           \t0.9470 +/- 0.0066 ✓ \t56.1956 +/- 120.3924\n",
      "Confidence level of 0.99 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.9911 +/- 0.0043 ✓ \t36.2722 +/- 5.8395\n",
      "\tscp_norm_var_50      \t0.9901 +/- 0.0035 ✓ \t29.1162 +/- 2.5530\n",
      "\tscp_norm_std_50      \t0.9905 +/- 0.0039 ✓ \t24.7139 +/- 2.7368\n",
      "\tscp_cqr_50           \t0.9907 +/- 0.0045 ✓ \t31.6276 +/- 5.7617\n",
      "\tscp_crf_50           \t0.9925 +/- 0.0028 ✓ \t108.5296 +/- 203.7387\n"
     ]
    }
   ],
   "source": [
    "# Get results for Split CP methods\n",
    "res_split = get_results_helper(DS_NAME, RES_PATH, split_calib_size=SPLIT_CALIB_SIZE, split=True)\n",
    "print_results(res_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a412cca1-ee04-40d2-a82b-6a83a57c862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9092 +/- 0.0089 ✓ \t9.3882 +/- 0.2465\n",
      "\tcrr_standard         \t0.9032 +/- 0.0082 ✓ \t10.5581 +/- 0.6230\n",
      "\tcrr_studentized      \t0.9045 +/- 0.0107 ✓ \t9.1491 +/- 0.2512\n",
      "\tcrr_deleted          \t0.9051 +/- 0.0097 ✓ \t9.4303 +/- 0.1896\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9418 +/- 0.0054 ✓ \t11.1867 +/- 0.2938\n",
      "\tcrr_standard         \t0.9531 +/- 0.0039 ✓ \t14.0365 +/- 0.8418\n",
      "\tcrr_studentized      \t0.9545 +/- 0.0063 ✓ \t12.2180 +/- 0.3524\n",
      "\tcrr_deleted          \t0.9557 +/- 0.0048 ✓ \t12.9851 +/- 0.2629\n",
      "Confidence level of 0.99 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9766 +/- 0.0029 ✗ \t14.7018 +/- 0.3861\n",
      "\tcrr_standard         \t0.9905 +/- 0.0021 ✓ \tinf +/- 0.0000\n",
      "\tcrr_studentized      \t0.9917 +/- 0.0015 ✓ \t20.6007 +/- 0.5013\n",
      "\tcrr_deleted          \t0.9925 +/- 0.0012 ✓ \t30.6101 +/- 1.7633\n"
     ]
    }
   ],
   "source": [
    "# Get results for Full CP methods\n",
    "res_full = get_results_helper(DS_NAME, RES_PATH, split=False)\n",
    "print_results(res_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbad5e0c-405d-4b85-a93a-28225e71afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9972 +/- 0.0010 ✗ \t33.5530 +/- 1.3632\n",
      "\tcrr_standard         \t0.9020 +/- 0.0118 ✓ \t19.0263 +/- 1.0602\n",
      "\tcrr_studentized      \t0.9012 +/- 0.0081 ✓ \t13.1022 +/- 0.2277\n",
      "\tcrr_deleted          \t0.8986 +/- 0.0113 ✓ \t13.4757 +/- 0.3004\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9978 +/- 0.0011 ✗ \t39.9809 +/- 1.6243\n",
      "\tcrr_standard         \t0.9539 +/- 0.0087 ✓ \t24.6442 +/- 1.6857\n",
      "\tcrr_studentized      \t0.9541 +/- 0.0063 ✓ \t16.7292 +/- 0.4239\n",
      "\tcrr_deleted          \t0.9518 +/- 0.0099 ✓ \t18.1909 +/- 0.8457\n",
      "Confidence level of 0.99 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9990 +/- 0.0013 ✗ \t52.5438 +/- 2.1347\n",
      "\tcrr_standard         \t0.9935 +/- 0.0027 ✓ \tinf +/- 0.0000\n",
      "\tcrr_studentized      \t0.9927 +/- 0.0032 ✓ \t27.5603 +/- 1.4054\n",
      "\tcrr_deleted          \t0.9899 +/- 0.0046 ✓ \tinf +/- 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get results for Full + Refine CP methods\n",
    "res_full_refine = get_results_helper(\n",
    "    DS_NAME, RES_PATH, split_calib_size=SPLIT_CALIB_SIZE, split=False\n",
    ")\n",
    "print_results(res_full_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4508076-374d-4976-beea-0977fc7ebceb",
   "metadata": {},
   "source": [
    "## Example Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1746735-7c4d-4db6-9a6a-256369a5eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SPLIT_CALIB_SIZE = 0.5\n",
    "BACKBONE = \"vgg19\"\n",
    "RES_PATH = f\"../results/pet_{BACKBONE}/\"  # Change the path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c03ac3a-aaaf-42ca-9b34-07710d54bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.85 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9211 +/- 0.0080 ✗ \t0.0071 +/- 0.0004\n",
      "\tcrr_standard         \t0.9087 +/- 0.0102 ✗ \t0.0040 +/- 0.0003\n",
      "\tcrr_studentized      \t0.9097 +/- 0.0088 ✗ \t0.0038 +/- 0.0002\n",
      "\tcrr_deleted          \t0.9097 +/- 0.0097 ✗ \t0.0041 +/- 0.0002\n",
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9449 +/- 0.0082 ✗ \t0.0094 +/- 0.0006\n",
      "\tcrr_standard         \t0.9411 +/- 0.0080 ✗ \t0.0061 +/- 0.0004\n",
      "\tcrr_studentized      \t0.9409 +/- 0.0091 ✗ \t0.0061 +/- 0.0003\n",
      "\tcrr_deleted          \t0.9420 +/- 0.0097 ✗ \t0.0069 +/- 0.0003\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes                \t0.9665 +/- 0.0070 ✗ \t0.0141 +/- 0.0008\n",
      "\tcrr_standard         \t0.9740 +/- 0.0057 ✗ \t0.0115 +/- 0.0008\n",
      "\tcrr_studentized      \t0.9741 +/- 0.0072 ✗ \t0.0122 +/- 0.0008\n",
      "\tcrr_deleted          \t0.9728 +/- 0.0076 ✗ \t0.0153 +/- 0.0009\n"
     ]
    }
   ],
   "source": [
    "res_full = get_results_helper_pets(RES_PATH, cp_type=\"full\")\n",
    "print_results(res_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc0e0e23-7438-4141-a433-b285796095cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.85 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes_50             \t0.9912 +/- 0.0033 ✗ \t0.0348 +/- 0.0024\n",
      "\tcrr_standard_50      \t0.8736 +/- 0.0160 ✗ \t0.0040 +/- 0.0003\n",
      "\tcrr_studentized_50   \t0.8778 +/- 0.0166 ✗ \t0.0036 +/- 0.0003\n",
      "\tcrr_deleted_50       \t0.8809 +/- 0.0167 ✗ \t0.0040 +/- 0.0003\n",
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes_50             \t0.9936 +/- 0.0033 ✗ \t0.0463 +/- 0.0032\n",
      "\tcrr_standard_50      \t0.9144 +/- 0.0143 ✗ \t0.0060 +/- 0.0006\n",
      "\tcrr_studentized_50   \t0.9175 +/- 0.0138 ✗ \t0.0056 +/- 0.0005\n",
      "\tcrr_deleted_50       \t0.9190 +/- 0.0131 ✗ \t0.0064 +/- 0.0006\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tbayes_50             \t0.9959 +/- 0.0024 ✗ \t0.0688 +/- 0.0048\n",
      "\tcrr_standard_50      \t0.9556 +/- 0.0083 ✓ \t0.0110 +/- 0.0011\n",
      "\tcrr_studentized_50   \t0.9562 +/- 0.0075 ✓ \t0.0110 +/- 0.0013\n",
      "\tcrr_deleted_50       \t0.9606 +/- 0.0076 ✗ \t0.0144 +/- 0.0021\n"
     ]
    }
   ],
   "source": [
    "res_full_refine = get_results_helper_pets(\n",
    "    RES_PATH, cp_type=\"full_refine\", split_calib_size=SPLIT_CALIB_SIZE\n",
    ")\n",
    "print_results(res_full_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d596d930-9087-496c-bec1-eea37c1f55f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence level of 0.85 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.8736 +/- 0.0121 ✗ \t0.0118 +/- 0.0010\n",
      "\tscp_norm_var_50      \t0.8628 +/- 0.0123 ✓ \t0.0151 +/- 0.0025\n",
      "\tscp_norm_std_50      \t0.8684 +/- 0.0130 ✗ \t0.0115 +/- 0.0011\n",
      "\tscp_cqr_50           \t0.8746 +/- 0.0137 ✗ \t0.0163 +/- 0.0012\n",
      "\tscp_crf_50           \t0.8748 +/- 0.0152 ✗ \t0.0125 +/- 0.0013\n",
      "Confidence level of 0.9 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.9187 +/- 0.0120 ✗ \t0.0172 +/- 0.0016\n",
      "\tscp_norm_var_50      \t0.9089 +/- 0.0121 ✓ \t0.0210 +/- 0.0034\n",
      "\tscp_norm_std_50      \t0.9135 +/- 0.0137 ✗ \t0.0162 +/- 0.0015\n",
      "\tscp_cqr_50           \t0.9142 +/- 0.0124 ✗ \t0.0274 +/- 0.0023\n",
      "\tscp_crf_50           \t0.9196 +/- 0.0111 ✗ \t0.0190 +/- 0.0020\n",
      "Confidence level of 0.95 \tAvg. Coverage \t\tAvg. Size\n",
      "\tscp_standard_50      \t0.9591 +/- 0.0087 ✗ \t0.0314 +/- 0.0036\n",
      "\tscp_norm_var_50      \t0.9554 +/- 0.0107 ✓ \t0.0334 +/- 0.0054\n",
      "\tscp_norm_std_50      \t0.9556 +/- 0.0092 ✓ \t0.0264 +/- 0.0024\n",
      "\tscp_cqr_50           \t0.9597 +/- 0.0062 ✗ \t0.0797 +/- 0.0071\n",
      "\tscp_crf_50           \t0.9602 +/- 0.0087 ✗ \t0.0378 +/- 0.0041\n"
     ]
    }
   ],
   "source": [
    "res_split = get_results_helper_pets(RES_PATH, cp_type=\"split\", split_calib_size=SPLIT_CALIB_SIZE)\n",
    "print_results(res_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
